{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582d93fe-feee-43b9-b22e-a52c3faafadd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_indicium = pd.read_csv(\"filmes_tratado.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd841391-04b5-42e0-90fc-dee14a01aaf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (999, 18)\n             count          mean  ...         75%          max\nIMDB_Rating  999.0  7.947948e+00  ...         8.1          9.2\nMeta_score   999.0  7.796912e+01  ...        85.5        100.0\nRuntime      999.0  1.228719e+02  ...       137.0        321.0\nNo_of_Votes  999.0  2.716214e+05  ...    373167.5    2303232.0\nGross        999.0  6.053338e+07  ...  61576564.5  936662225.0\n\n[5 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Criar coluna primary_genre (primeiro gênero listado)\n",
    "df_indicium['primary_genre'] = df_indicium['Genre'].astype(str).str.split(',').str[0].str.strip()\n",
    "\n",
    "# Criar logs (No_of_Votes/Gross)\n",
    "df_indicium['log_No_of_Votes'] = np.log1p(df_indicium['No_of_Votes'])\n",
    "df_indicium['log_Gross'] = np.log1p(df_indicium['Gross'])\n",
    "\n",
    "# Verificações rápidas\n",
    "print(\"Shape:\", df_indicium.shape)\n",
    "print(df_indicium[['IMDB_Rating','Meta_score','Runtime','No_of_Votes','Gross']].describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7826fbdf-a023-476d-a857-ff4789571347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n  Downloading xgboost-3.0.4-py3-none-manylinux_2_28_aarch64.whl.metadata (2.0 kB)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.12/site-packages (from xgboost) (1.26.4)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.12/site-packages (from xgboost) (1.13.1)\nDownloading xgboost-3.0.4-py3-none-manylinux_2_28_aarch64.whl (4.6 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/4.6 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.6/4.6 MB\u001B[0m \u001B[31m60.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: xgboost\nSuccessfully installed xgboost-3.0.4\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Pacote para XGBoost\n",
    "%pip install xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f0d8800-9aeb-4ebc-8e3e-b6bf4efb2fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: click in /databricks/python3/lib/python3.12/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.12/site-packages (from nltk) (1.4.2)\nCollecting regex>=2021.8.3 (from nltk)\n  Downloading regex-2025.9.1-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl.metadata (40 kB)\nCollecting tqdm (from nltk)\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m9.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading regex-2025.9.1-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl (797 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/797.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m797.4/797.4 kB\u001B[0m \u001B[31m12.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nInstalling collected packages: tqdm, regex, nltk\nSuccessfully installed nltk-3.9.1 regex-2025.9.1 tqdm-4.67.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Pacote para NLTK\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccdad8d6-d3a8-40ff-a1ca-ad5a1cb39b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0xff08bc91dd00>\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.12/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/databricks/python/lib/python3.12/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n                   ^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\n             ^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'split'\n[nltk_data] Downloading package punkt to\n[nltk_data]     /home/spark-96ea654b-0c9d-4127-9916-ee/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/spark-96ea654b-0c9d-4127-9916-ee/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /home/spark-96ea654b-0c9d-4127-9916-ee/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Downloads necessários do nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Função de pré-processamento do texto\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # remove pontuação/números\n",
    "    tokens = [t for t in tokens if t not in stop_words]  # remove stopwords\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Aplica o pré-processamento ao texto\n",
    "df_indicium[\"Overview\"] = df_indicium[\"Overview\"].apply(preprocess_text)\n",
    "df_indicium[\"Series_Title\"] = df_indicium[\"Series_Title\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba81ccc6-d719-4f05-995c-37016eecbc0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Separa features e target\n",
    "X = df_indicium.drop(columns=[\"IMDB_Rating\"])\n",
    "y = df_indicium[\"IMDB_Rating\"]\n",
    "\n",
    "# Features numéricas e categóricas\n",
    "numeric_features = ['Released_Year', 'Runtime', 'Meta_score', 'log_No_of_Votes', 'log_Gross']\n",
    "categorical_features = ['Certificate', 'primary_genre', 'Director', 'Star1', 'Star2', 'Star3', 'Star4']\n",
    "text_features = [\"Overview\", \"Series_Title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "568434d8-ed83-4365-a7af-01dba0802d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transformers\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "text_transformer = Pipeline(steps=[\n",
    "    (\"tfidf\", TfidfVectorizer(max_features=5000))\n",
    "])\n",
    "\n",
    "# Column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "        (\"text1\", text_transformer, \"Overview\"),\n",
    "        (\"text2\", text_transformer, \"Series_Title\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Dicionário de pipelines para cada modelo\n",
    "models = {\n",
    "    \"RandomForest\": Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", RandomForestRegressor(random_state=42))\n",
    "    ]),\n",
    "    \"XGBoost\": Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", XGBRegressor(random_state=42, objective=\"reg:squarederror\", n_jobs=-1))\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c625d8d2-98a8-4f10-a6eb-23e09b1e03c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Separando os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b57443-7fb0-4bbc-883c-935f963a3eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nRodando GridSearch para RandomForest...\nMelhores hiperparâmetros (RandomForest): {'model__max_depth': None, 'model__n_estimators': 500}\nMAE: 0.1555 | RMSE: 0.2028 | R²: 0.3735\n\nRodando GridSearch para XGBoost...\nMelhores hiperparâmetros (XGBoost): {'model__learning_rate': 0.1, 'model__max_depth': 5, 'model__n_estimators': 100}\nMAE: 0.1617 | RMSE: 0.2070 | R²: 0.3472\n\nComparativo final:\n         Modelo  ...                           Melhores Hiperparâmetros\n0  RandomForest  ...  {'model__max_depth': None, 'model__n_estimator...\n1       XGBoost  ...  {'model__learning_rate': 0.1, 'model__max_dept...\n\n[2 rows x 5 columns]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+display_df_hint": {
       "mode": "should_hint",
       "name": "results_df",
       "type": "pandas.core.frame.DataFrame"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dicionário de grades de hiperparâmetros\n",
    "param_grids = {\n",
    "    \"RandomForest\": {\n",
    "        \"model__n_estimators\": [100, 200, 500],\n",
    "        \"model__max_depth\": [5, 10, None]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model__n_estimators\": [100, 200, 500],\n",
    "        \"model__max_depth\": [3, 5, 7],\n",
    "        \"model__learning_rate\": [0.05, 0.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# loop para treinar ambos os modelos\n",
    "results = []\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"\\nRodando GridSearch para {name}...\")\n",
    "    grid = GridSearchCV(pipeline, param_grids[name], cv=3, scoring=\"neg_mean_absolute_error\")\n",
    "    grid.fit(X_train, y_train)\n",
    "    y_pred = grid.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Melhores hiperparâmetros ({name}): {grid.best_params_}\")\n",
    "    print(f\"MAE: {mae:.4f} | RMSE: {rmse:.4f} | R²: {r2:.4f}\")\n",
    "    \n",
    "    results.append({\n",
    "        \"Modelo\": name,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"Melhores Hiperparâmetros\": grid.best_params_\n",
    "    })\n",
    "\n",
    "# Tabela comparativa\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nComparativo final:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9e296d-1c7b-42e8-b36d-be7c24ed7721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Modelo</th><th>MAE</th><th>RMSE</th><th>R2</th><th>Melhores Hiperparâmetros</th></tr></thead><tbody><tr><td>RandomForest</td><td>0.15547099999999925</td><td>0.20279971942781508</td><td>0.3734731856681418</td><td>List(null, null, 500)</td></tr><tr><td>XGBoost</td><td>0.1617183117866516</td><td>0.20700512243660396</td><td>0.3472195369722537</td><td>List(0.1, 5, 100)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "RandomForest",
         0.15547099999999925,
         0.20279971942781508,
         0.3734731856681418,
         [
          null,
          null,
          500
         ]
        ],
        [
         "XGBoost",
         0.1617183117866516,
         0.20700512243660396,
         0.3472195369722537,
         [
          0.1,
          5,
          100
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Modelo",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "MAE",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "RMSE",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "R2",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Melhores Hiperparâmetros",
         "type": "{\"fields\":[{\"metadata\":{},\"name\":\"model__learning_rate\",\"nullable\":true,\"type\":\"double\"},{\"metadata\":{},\"name\":\"model__max_depth\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"model__n_estimators\",\"nullable\":true,\"type\":\"long\"}],\"type\":\"struct\"}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69206b4c-8929-4561-af6b-e2684c29ec1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDFAC Nota prevista no IMDB para The Shawshank Redemption: 8.68\n"
     ]
    }
   ],
   "source": [
    "# Dados para previsão\n",
    "teste = {\n",
    "    'Series_Title': 'The Shawshank Redemption',\n",
    "    'Released_Year': 1994,\n",
    "    'Certificate': 'A',\n",
    "    'Runtime': '142 min',\n",
    "    'Genre': 'Drama',\n",
    "    'Overview': 'Two imprisoned men bond over a number of years, finding solace and eventual redemption through acts of common decency.',\n",
    "    'Meta_score': 80.0,\n",
    "    'Director': 'Frank Darabont',\n",
    "    'Star1': 'Tim Robbins',\n",
    "    'Star2': 'Morgan Freeman',\n",
    "    'Star3': 'Bob Gunton',\n",
    "    'Star4': 'William Sadler',\n",
    "    'No_of_Votes': 2343110,\n",
    "    'Gross': '28,341,469'\n",
    "}\n",
    "\n",
    "# Transformar em DataFrame\n",
    "df_teste = pd.DataFrame([teste])\n",
    "\n",
    "# Ajustar campos numéricos\n",
    "df_teste['Released_Year'] = df_teste['Released_Year'].astype(int)\n",
    "df_teste['Runtime'] = df_teste['Runtime'].str.replace(' min', '').astype(float)\n",
    "df_teste['Gross'] = df_teste['Gross'].str.replace(',', '').astype(float)\n",
    "\n",
    "# Criar colunas derivadas\n",
    "df_teste['log_No_of_Votes'] = np.log1p(df_teste['No_of_Votes'])\n",
    "df_teste['log_Gross'] = np.log1p(df_teste['Gross'])\n",
    "\n",
    "# Criar coluna primary_genre (igual ao treino)\n",
    "df_teste['primary_genre'] = df_teste['Genre'].str.split(',').str[0]\n",
    "\n",
    "# Aplicar o mesmo pré-processamento de texto\n",
    "df_teste[\"Overview\"] = df_teste[\"Overview\"].apply(preprocess_text)\n",
    "\n",
    "# Manter as colunas do treino\n",
    "df_teste = df_teste[X.columns]\n",
    "\n",
    "# Usar o melhor modelo RandomForest já treinado\n",
    "best_rf = grid  # ou grid.best_estimator_ se for o objeto GridSearch\n",
    "pred = best_rf.predict(df_teste)[0]\n",
    "\n",
    "print(f\"\uD83C\uDFAC Nota prevista no IMDB para {teste['Series_Title']}: {pred:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d256fe7e-5f2b-4e2b-a44b-90a3c91f3f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo salvo como modelo_randomforest.pkl\n\uD83C\uDFAC Previsão carregada do modelo salvo: 8.68\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# salvar o melhor modelo RandomForest treinado\n",
    "joblib.dump(grid.best_estimator_, \"modelo_randomforest.pkl\")\n",
    "\n",
    "print(\"✅ Modelo salvo como modelo_randomforest.pkl\")\n",
    "\n",
    "# exemplo de como carregar depois\n",
    "modelo_carregado = joblib.load(\"modelo_randomforest.pkl\")\n",
    "\n",
    "# testar previsão novamente\n",
    "pred = modelo_carregado.predict(df_teste)[0]\n",
    "print(f\"\uD83C\uDFAC Previsão carregada do modelo salvo: {pred:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Construção e treinamento dos modelos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}